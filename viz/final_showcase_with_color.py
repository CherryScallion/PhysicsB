#!/usr/bin/env python3
"""
ã€æœ€ç»ˆå¯è§†åŒ–è„šæœ¬ã€‘æ¨¡å‹é¢„æµ‹ç»“æœçš„ 3D æ¸²æŸ“ä¸å¯¹æ¯”
- ä½¿ç”¨ç»è¿‡éªŒè¯çš„ç‰©ç†ä»¿å°„çŸ©é˜µ (Fixing coordinates)
- ä½¿ç”¨ MNI é‡é‡‡æ ·å¹³æ»‘åƒç´  (Smoother blobs)
- ä½¿ç”¨çº¢è“é…è‰²ä¸é˜ˆå€¼åˆ‡å‰² (Professional visualization)
"""

import sys
from pathlib import Path
import torch
import numpy as np
import nibabel as nib
import matplotlib.pyplot as plt
from nilearn import plotting, datasets, image
import os
import warnings

# --- è·¯å¾„è®¾ç½® ---
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° sys.path
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

from data.loaders import FMRIEEGDataset
from models.classifier_net import PhysicsE2fNet
from utils.paths import get_config_path, get_template_dir, get_checkpoint_dir, resolve_path

# å¿½ç•¥ nilearn çš„ä¸€äº› warning
warnings.filterwarnings("ignore")

# --- è¾“å‡ºé…ç½® ---
OUTPUT_DIR = resolve_path("./results/final_showcase_color")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# è‡ªåŠ¨åŠ è½½ MNI æ¨¡æ¿ç”¨äºé‡é‡‡æ ·èƒŒæ™¯
MNI_TEMPLATE = datasets.load_mni152_template()

def compute_optimized_affine(src_shape):
    """
    [æ ¸å¿ƒä¿®å¤]: è®¡ç®—ä¸€ä¸ªç‰©ç†ä»¿å°„çŸ©é˜µ
    å°†æˆ‘ä»¬çš„ä½åˆ†æ•°æ® (e.g. 64x64x30) æ˜ å°„åˆ°æ ‡å‡†å¤§è„‘ç‰©ç†å°ºå¯¸ (mm)ã€‚
    è¿™è§£å†³äº†â€œé©¬èµ›å…‹â€å’Œâ€œæ˜¾ç¤ºé”™ä½â€é—®é¢˜ã€‚
    """
    # ç›®æ ‡ç‰©ç†è¦†ç›–èŒƒå›´ (mm) - é€‚é…æ ‡å‡†æˆäººå¤§è„‘
    TARGET_FOV = (180.0, 200.0, 95.0) 
    
    # src_shape: [64, 64, 30]
    sx = TARGET_FOV[0] / src_shape[0]
    sy = TARGET_FOV[1] / src_shape[1]
    sz = TARGET_FOV[2] / src_shape[2]
    
    # ç¼©æ”¾
    affine = np.diag([1.12 * sx, sy, 1.4*sz, 1.0])# 1.2 was a little bit too big, see ex5.png
    
    # å¹³ç§» (å°†æ•°æ®ä¸­å¿ƒå¯¹é½åˆ° MNI åæ ‡åŸç‚¹)
    tx = -sx * (src_shape[0] / 2) - 10 # was 180 * ((1.2 - 2)/2), now 0.12/2 * 180, and a little instinct
    ty = -sy * (src_shape[1] / 2) - 14 # no reason but instinct
    tz = -sz * (src_shape[2] / 2) + 0 # Z axis
    
    affine[:3, 3] = [tx, ty, tz]
    return affine

def reconstruct_mni_img(weights, ica_basis, mask_bool):
    """
    ä¸€æ­¥åˆ°ä½ï¼šæƒé‡ -> 3Dä½“ç§¯ -> æ­£ç¡®åæ ‡ç³» -> MNIé«˜æ¸…é‡é‡‡æ ·
    """
    # 1. ç»´åº¦å¤„ç†ä¸è½¬æ¢ (Tensor -> Numpy)
    if isinstance(weights, torch.Tensor): weights = weights.detach().cpu()
    if isinstance(ica_basis, torch.Tensor): ica_basis = ica_basis.detach().cpu()
    
    if weights.dim() == 1: weights = weights.unsqueeze(0)
    
    # å¤„ç†åŸºåº•ç»´åº¦ [K, D, H, W] -> Flatten [K, Voxels]
    n_mask_voxels = mask_bool.sum()
    
    if ica_basis.dim() == 4:
        # [K, D, H, W]
        ica_basis_flat = ica_basis.reshape(ica_basis.shape[0], -1)
        if ica_basis_flat.shape[1] != n_mask_voxels:
            # éœ€è¦ Apply Mask
            mask_flat = mask_bool.reshape(-1)
            ica_basis = ica_basis_flat[:, mask_flat]
        else:
            ica_basis = ica_basis_flat
            
    # 2. çŸ©é˜µä¹˜æ³•é‡å»º [1, K] @ [K, V] -> [1, V]
    # ä½¿ç”¨ float32 é¿å…ç²¾åº¦é—®é¢˜
    activation_vec = torch.matmul(weights.float(), ica_basis.float()).numpy().flatten()
    
    # 3. å¡«å…¥ 3D ç›’å­
    vol_data = np.zeros(mask_bool.shape) # [D, H, W] (30, 64, 64)
    vol_data[mask_bool] = activation_vec
    
    # 4. [å…³é”®] è½´è½¬ç½® (2, 1, 0)
    # å°† [D, H, W] (30, 64, 64) -> [X, Y, Z] (64, 64, 30)
    vol_nii_data = np.transpose(vol_data, (1, 2, 0)) # AI sucks, I fix it myself
    
    # 5. [å…³é”®] èµ‹äºˆç‰©ç†åæ ‡
    affine = compute_optimized_affine(vol_nii_data.shape)
    raw_img = nib.Nifti1Image(vol_nii_data, affine)
    
    # 6. MNI ç©ºé—´é‡é‡‡æ · (æ’å€¼å¹³æ»‘)
    # è¿™ä¸€æ­¥æŠŠæ–¹å—å˜æˆäº†çœ‹èµ·æ¥å¾ˆçœŸçš„è„‘å›¾
    smooth_img = image.resample_to_img(raw_img, MNI_TEMPLATE, interpolation='continuous')
    
    return smooth_img

def main():
    print("ğŸš€ Starting Final Visualization pipeline...")
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # 1. å¯»æ‰¾å¿…è¦æ–‡ä»¶
    template_dir = get_template_dir()
    
    ica_path = next((p for p in [template_dir/"ica_mixing_matrix.pt", template_dir/"ica_basis.pt"] if p.exists()), None)
    mask_path = next((p for p in [template_dir/"mask_dhw.pt", template_dir/"gray_mask.pt"] if p.exists()), None)
    
    model_path = get_checkpoint_dir() / "model_ep50.pth" # ç¡®ä¿è¯»å–è®­ç»ƒå¥½çš„æƒé‡
    
    if not (ica_path and mask_path and model_path.exists()):
        print(f"âŒ Critical files missing.")
        print(f"Model: {model_path} ({model_path.exists()})")
        return

    # 2. åŠ è½½åŸºåº•
    basis = torch.load(ica_path, map_location='cpu')
    mask = torch.load(mask_path, map_location='cpu').numpy().astype(bool)
    if mask.ndim == 4: mask = mask.squeeze() # å®¹é”™
    
    print(f"âœ… Assets Loaded. Basis: {basis.shape}")

    # 3. åˆå§‹åŒ–æ•°æ®é›†å’Œæ¨¡å‹
    config_path = get_config_path()
    # å¿…é¡»ç”¨ lazy_load=True é˜²æ­¢åŠ è½½æ‰€æœ‰æ•°æ®å¤ªæ…¢ï¼Œåæ­£æˆ‘ä»¬åªå–å‡ ä¸ª
    ds = FMRIEEGDataset(config_path=str(config_path), lazy_load=True)
    sample_eeg, _ = ds[0]
    
    model = PhysicsE2fNet(
        n_ica_components=64,
        eeg_channels=sample_eeg.shape[0], # 20
        eeg_time_len=sample_eeg.shape[2], # 249
        basis_path=str(ica_path),
        task='regression'
    ).to(device)
    
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    print("âœ… Model loaded.")

    # 4. é€‰æ‹©æ ·æœ¬å¹¶ç»˜å›¾
    # é€‰å‡ ä¸ª Validation é›†çš„æ ·æœ¬ (é åçš„ç´¢å¼•)
    indices = [len(ds)-10, len(ds)-50, len(ds)-100]
    indices = [i for i in indices if i >= 0]
    
    if not indices: indices = [0] # å…œåº•

    print(f"Visualizing indices: {indices}")

    for idx in indices:
        eeg, gt_weights = ds[idx]
        
        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            input_t = eeg.unsqueeze(0).to(device).float()
            pred_weights = model(input_t).cpu()
            
        # --- é‡å»ºä¸ºé«˜æ¸…è„‘å›¾ ---
        img_pred = reconstruct_mni_img(pred_weights, basis, mask)
        img_gt = reconstruct_mni_img(gt_weights, basis, mask)
        
        # --- å¯è§†åŒ–æ ¸å¿ƒå‚æ•° ---
        # è‡ªåŠ¨è®¡ç®—å¯¹æ¯”åº¦èŒƒå›´
        # ä½¿ç”¨çœŸå€¼çš„ 99% åˆ†ä½æ•°ï¼Œä¿è¯ Prediction å’Œ GT å…±äº«åŒä¸€è‰²æ ‡ï¼Œå®ç°å…¬å¹³å¯¹æ¯”
        vmax = np.percentile(np.abs(img_gt.get_fdata()), 99.8)
        # è®¾å®šåº•å™ªé˜ˆå€¼ (éšè—æ‰ 25% ä»¥ä¸‹çš„å¾®å¼±ä¿¡å·ï¼Œåªç•™çº¢è“ä¸»æˆåˆ†)
        thresh = vmax * 0.25 
        
        # ç»˜å›¾
        fig, axes = plt.subplots(2, 1, figsize=(10, 8))
        
        # Prediction
        plotting.plot_stat_map(
            img_pred, bg_img=MNI_TEMPLATE, 
            display_mode='z', cut_coords=[-40, -20, 0, 20, 40], # å›ºå®š5ä¸ªè½´å‘åˆ‡ç‰‡
            threshold=thresh, vmax=vmax,    # ç»Ÿä¸€åº¦é‡è¡¡
            cmap='cold_hot',                # çº¢è“ä¸“ä¸šé…è‰²
            title=f"Sample {idx}: EEG Prediction",
            axes=axes[0], colorbar=True
        )
        
        # Ground Truth
        plotting.plot_stat_map(
            img_gt, bg_img=MNI_TEMPLATE, 
            display_mode='z', cut_coords=[-40, -20, 0, 20, 40], 
            threshold=thresh, vmax=vmax, 
            cmap='cold_hot',
            title=f"Sample {idx}: fMRI Ground Truth",
            axes=axes[1], colorbar=True
        )
        
        save_file = OUTPUT_DIR / f"Compare_Sample_{idx}.png"
        fig.savefig(save_file, dpi=300, bbox_inches='tight')
        plt.close(fig)
        print(f"ğŸ“¸ Saved: {save_file.name} (Max Intensity: {vmax:.2f})")

    print(f"\nğŸ‰ Visualization Done. Results in {OUTPUT_DIR}")

if __name__ == "__main__":
    main()