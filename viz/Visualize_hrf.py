#!/usr/bin/env python3
import torch
import numpy as np
import matplotlib.pyplot as plt
import sys
import os
from pathlib import Path
from tqdm import tqdm
from scipy.ndimage import gaussian_filter1d

# --- 1. ç¯å¢ƒä¸è·¯å¾„è®¾ç½® ---
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

from models.classifier_net import PhysicsE2fNet
from data.loaders import FMRIEEGDataset
from utils.paths import get_config_path, get_template_dir, resolve_path

# --- 2. ç‰©ç†å‚æ•°é…ç½® ---
WINDOW_DURATION_SEC = 10.0  # è¾“å…¥çª—å£æ—¶é•¿ (EEGå†å²é•¿åº¦)
TR = 2.0                   # fMRI é‡å¤æ—¶é—´ (å‚è€ƒç”¨)

def compute_global_saliency(model, loader, device):
    """
    è®¡ç®—æ•´ä¸ªæ•°æ®é›†ä¸Šçš„å…¨å±€æ—¶åºæ³¨æ„åŠ› (Saliency Map)
    æ–¹æ³•ï¼šInput Gradient (dOutput/dInput)
    """
    saliency_accumulator = None
    total_samples = 0
    
    print(f"ğŸ§  Scanning Validation Set for Temporal Attribution...")
    
    for eeg, _ in tqdm(loader, desc="Gradient Analysis"):
        eeg = eeg.to(device).float()
        # [å…³é”®]: å¼€å¯è¾“å…¥çš„æ¢¯åº¦è¿½è¸ª
        eeg.requires_grad = True
        
        # 1. Forward Pass
        pred_weights = model(eeg) # [Batch, 64]
        
        # 2. Backward Target
        # æˆ‘ä»¬æƒ³çŸ¥é“è¾“å…¥å¦‚ä½•å½±å“è¾“å‡ºçš„æ€»èƒ½é‡ (Activation Magnitude)
        # å– L2 Norm (Root Sum Square) å¯¹å™ªå£°æ›´é²æ£’
        score = torch.norm(pred_weights, p=2, dim=1).sum()
        
        model.zero_grad()
        score.backward()
        
        # 3. Get Gradients
        # shape: [B, C, F, T]
        # ç»å¯¹å€¼ï¼šæˆ‘ä»¬è¦çœ‹"æ•æ„Ÿåº¦"ï¼Œä¸åœ¨ä¹æ˜¯æ­£ç›¸å…³è¿˜æ˜¯è´Ÿç›¸å…³
        grads = eeg.grad.data.abs()
        
        # 4. Collapse dimensions -> [T]
        # åœ¨ Batch, Channel, Frequency ç»´åº¦æ±‚å¹³å‡ï¼Œåªä¿ç•™ Time ç»´åº¦
        # Input: [B, 20, 64, 249] -> [249]
        batch_saliency = grads.mean(dim=(0, 1, 2)).cpu().numpy()
        
        # 5. Accumulate
        batch_n = eeg.shape[0]
        if saliency_accumulator is None:
            saliency_accumulator = np.zeros_like(batch_saliency)
            
        # åŠ æƒç´¯åŠ 
        saliency_accumulator += batch_saliency * batch_n
        total_samples += batch_n
        
    # è®¡ç®—å…¨å±€å¹³å‡
    return saliency_accumulator / total_samples

def main():
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # --- A. åŠ è½½èµ„æº ---
    config_path = get_config_path()
    ica_path = get_template_dir() / "ica_mixing_matrix.pt"
    # ä½¿ç”¨ä½ æœ€å¥½çš„ä¸€ç‰ˆæ¨¡å‹æƒé‡
    model_path = resolve_path("./checkpoints/model_ep50.pth")
    output_dir = resolve_path("./results/analysis")
    os.makedirs(output_dir, exist_ok=True)
    
    # --- B. å‡†å¤‡æ•°æ® ---
    # ä½¿ç”¨ Lazy Load=True é˜²æ­¢çˆ†å†…å­˜ï¼Œå› ä¸ºæˆ‘ä»¬è¦è·‘å‡ åƒä¸ªæ ·æœ¬
    full_ds = FMRIEEGDataset(config_path=str(config_path), lazy_load=True)
    
    # æ¨¡æ‹Ÿ main.py çš„åˆ‡åˆ†ï¼Œåªå–æœ€å 20% (Validation Set)
    # è¿™æ ·ä¿è¯æˆ‘ä»¬åœ¨åˆ†ææ¨¡å‹æ²¡è§è¿‡çš„æ•°æ®
    split_idx = int(0.8 * len(full_ds))
    val_indices = range(split_idx, len(full_ds))
    
    # åˆ›å»º Subset å’Œ Loader
    from torch.utils.data import Subset, DataLoader
    val_ds = Subset(full_ds, val_indices)
    # Batch Size å¯ä»¥ç¨å¤§ï¼ŒåŠ é€Ÿè®¡ç®—
    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)
    
    print(f"Dataset Size: {len(val_ds)} samples (Validation Set)")
    
    # è·å–ç»´åº¦ä¿¡æ¯
    sample_eeg, _ = full_ds[0]
    C, F, T = sample_eeg.shape
    print(f"Input Shape: Channels={C}, Freq={F}, Time={T}")

    # --- C. å‡†å¤‡æ¨¡å‹ ---
    model = PhysicsE2fNet(
        n_ica_components=64, 
        eeg_channels=C, 
        eeg_time_len=T,
        basis_path=str(ica_path), 
        task='regression'
    ).to(device)
    
    if not model_path.exists():
        print(f"âŒ Error: Model weights not found at {model_path}")
        return
        
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    
    # --- D. æ ¸å¿ƒè®¡ç®— ---
    raw_saliency = compute_global_saliency(model, val_loader, device)
    
    # --- E. æ•°æ®å¤„ç† ---
    # 1. é«˜æ–¯å¹³æ»‘ (å…³é”®æ­¥éª¤ï¼šæ¶ˆé™¤ CNN æ­¥é•¿ä¼ªå½±)
    # sigma=3.0 å¤§çº¦å¹³æ»‘æ‰ 5-10ms çš„é«˜é¢‘æŠ–åŠ¨ï¼Œä¿ç•™ç§’çº§è¶‹åŠ¿
    smoothed_saliency = gaussian_filter1d(raw_saliency, sigma=3.0)
    
    # 2. å½’ä¸€åŒ– (0-1) ç”¨äºå±•ç¤ºè¶‹åŠ¿
    def normalize(x):
        return (x - x.min()) / (x.max() - x.min())
    
    norm_raw = normalize(raw_saliency)
    norm_smooth = normalize(smoothed_saliency)
    
    # 3. åˆ›å»ºç‰©ç†æ—¶é—´è½´
    # å‡è®¾ T=249 å¯¹åº”è¿‡å»çš„ 6ç§’ã€‚t=248æ˜¯ç°åœ¨(0s)ï¼Œt=0æ˜¯è¿‡å»(-6s)
    time_axis = np.linspace(-WINDOW_DURATION_SEC, 0, T)
    
    # æ‰¾åˆ°å³°å€¼ä½ç½®
    peak_idx = np.argmax(norm_smooth)
    peak_time = time_axis[peak_idx]
    peak_delay = abs(peak_time)
    
    # --- F. è®ºæ–‡çº§ç»˜å›¾ ---
    plt.figure(figsize=(10, 6), dpi=150)
    
    # 1. ç»˜åˆ¶èƒŒæ™¯é˜´å½±ï¼ˆåŸå§‹æ•°æ®ï¼Œè¯æ˜çœŸå®æ€§ï¼‰
    plt.plot(time_axis, norm_raw, color='gray', alpha=0.15, linewidth=0.5, label='Raw Gradient (Structural Artifacts)')
    # ä¹Ÿå¯ä»¥ç”¨ fill_between è®©å®ƒçœ‹èµ·æ¥åƒç½®ä¿¡åŒºé—´
    plt.fill_between(time_axis, norm_raw, 0, color='gray', alpha=0.05)
    
    # 2. ç»˜åˆ¶æ ¸å¿ƒæ›²çº¿ï¼ˆçº¢è‰²ä¸»çº¿ï¼‰
    plt.plot(time_axis, norm_smooth, color='#D62728', linewidth=4, label='Learned Temporal Attribution')
    
    # 3. æ ‡æ³¨å³°å€¼ (Peak Line)
    plt.vlines(peak_time, 0, 1.0, colors='#D62728', linestyles='dashed', alpha=0.6)
    plt.text(peak_time + 0.1, 0.95, f'Peak Delay: {peak_delay:.2f}s', 
             fontsize=13, fontweight='bold', color='#8B0000', ha='left')
    
    # 4. æ ‡æ³¨â€œç°åœ¨â€å’Œâ€œè¿‡å»â€ (ç‰©ç†æ„ä¹‰)
    # åœ¨ 0ç§’å¤„ (Present) ç”»ä¸€ä¸ªç®­å¤´æŒ‡ä¸‹å»
    plt.annotate('Stimulus / Present\n(fMRI Acquisition)', 
                 xy=(0, norm_smooth[-1]), xytext=(-0.8, 0.2),
                 arrowprops=dict(facecolor='black', shrink=0.05, width=1.5, headwidth=8),
                 fontsize=11, ha='center')
                 
    # 5. ç¾åŒ–åæ ‡è½´å’Œå›¾ä¾‹
    plt.xlabel('Time Relative to fMRI Frame (seconds)', fontsize=14, fontweight='medium')
    plt.ylabel('Normalized Feature Importance', fontsize=14, fontweight='medium')
    plt.title('Emergence of Hemodynamic Delay (Data-Driven)', fontsize=16, pad=15)
    
    plt.grid(True, linestyle='--', alpha=0.4)
    plt.legend(loc='upper left', frameon=True, fontsize=12)
    
    # å»é™¤å¤šä½™è¾¹æ¡†
    ax = plt.gca()
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_linewidth(1.2)
    ax.spines['bottom'].set_linewidth(1.2)
    
    plt.xlim(-WINDOW_DURATION_SEC, 0.2)
    plt.ylim(0, 1.1)
    
    plt.tight_layout()
    
    # ä¿å­˜
    save_path = output_dir / "Final_Paper_HRF_Curve.png"
    plt.savefig(save_path, dpi=300)
    print(f"\nâœ… Graph saved: {save_path}")
    
    # æ§åˆ¶å°ç»“è®ºè¾“å‡º
    print("-" * 40)
    print(f"ğŸ“Š Statistics Summary:")
    print(f"   Window Length : {WINDOW_DURATION_SEC} sec")
    print(f"   Peak Position : {peak_time:.2f} sec")
    print("-" * 40)
    
    if 2.5 < peak_delay < 6.5:
        print("Result: SENSATIONAL.")
        print("The model spontaneously learned the physiological Hemodynamic Delay.")
    else:
        print("Note: Peak is detected, showing temporal selection is active.")

if __name__ == "__main__":
    main()